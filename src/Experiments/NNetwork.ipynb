{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592e13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d544213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['flat_model']\n",
    "numerical_cols = ['time', 'storey_avg', 'floor_area_sqm', 'flat_type_encoded', 'remaining_lease_months']\n",
    "\n",
    "# Create column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97dabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../data/processed/train.csv')\n",
    "test_data = pd.read_csv('../../data/processed/test.csv')\n",
    "\n",
    "X_train = train_data[numerical_cols + categorical_cols]\n",
    "y_train = train_data['resale_price']\n",
    "X_test = test_data[numerical_cols + categorical_cols]\n",
    "y_test = test_data['resale_price']\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454e1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network input dimension: 26\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_processed.toarray())\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test_processed.toarray())\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "# Create Dataset class\n",
    "class HDBDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = HDBDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = HDBDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get input dimension\n",
    "input_dim = X_train_processed.shape[1]\n",
    "print(f\"Neural network input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52cba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            \n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = NeuralNetwork(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e195e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "    \n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "    \n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d7a5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 294941841322.6673, Val Loss: 293296671373.9879\n",
      "Epoch 2/100, Train Loss: 292596916775.0331, Val Loss: 289755305893.1353\n",
      "Epoch 3/100, Train Loss: 288674662767.5906, Val Loss: 283025060782.0085\n",
      "Epoch 4/100, Train Loss: 283337148302.1029, Val Loss: 277335325885.7249\n",
      "Epoch 5/100, Train Loss: 276668714387.8918, Val Loss: 271797137670.7490\n",
      "Epoch 6/100, Train Loss: 268765548421.1824, Val Loss: 262610310993.3566\n",
      "Epoch 7/100, Train Loss: 259699304038.0488, Val Loss: 244217881247.4836\n",
      "Epoch 8/100, Train Loss: 249595090130.9801, Val Loss: 257488622044.2091\n",
      "Epoch 9/100, Train Loss: 238612324149.2585, Val Loss: 238143528044.7054\n",
      "Epoch 10/100, Train Loss: 226824235950.9551, Val Loss: 229098473342.8830\n",
      "Epoch 11/100, Train Loss: 214370996794.3791, Val Loss: 222894646077.0076\n",
      "Epoch 12/100, Train Loss: 201403621072.9224, Val Loss: 195614920160.3165\n",
      "Epoch 13/100, Train Loss: 188161514497.8891, Val Loss: 192549708352.1666\n",
      "Epoch 14/100, Train Loss: 174655045815.3641, Val Loss: 175724436779.8412\n",
      "Epoch 15/100, Train Loss: 161261591258.6503, Val Loss: 181593722896.1788\n",
      "Epoch 16/100, Train Loss: 147935345415.8642, Val Loss: 133339090744.4613\n",
      "Epoch 17/100, Train Loss: 134784444044.9290, Val Loss: 129159672710.1258\n",
      "Epoch 18/100, Train Loss: 121778080635.5956, Val Loss: 125285681954.4819\n",
      "Epoch 19/100, Train Loss: 108807196989.0933, Val Loss: 95830741321.4710\n",
      "Epoch 20/100, Train Loss: 96128834122.6681, Val Loss: 99628744463.1990\n",
      "Epoch 21/100, Train Loss: 83759393648.8722, Val Loss: 83932522957.8488\n",
      "Epoch 22/100, Train Loss: 71846057186.6889, Val Loss: 63209543472.9989\n",
      "Epoch 23/100, Train Loss: 60687873499.4910, Val Loss: 127805829343.3680\n",
      "Epoch 24/100, Train Loss: 50234953637.9170, Val Loss: 1590033286590.2656\n",
      "Epoch 25/100, Train Loss: 40684184431.4455, Val Loss: 30669514355.5563\n",
      "Epoch 26/100, Train Loss: 32262887553.5746, Val Loss: 34119560565.2102\n",
      "Epoch 27/100, Train Loss: 25053197459.6273, Val Loss: 17251592162.0880\n",
      "Epoch 28/100, Train Loss: 19223537406.2304, Val Loss: 10456762199.5334\n",
      "Epoch 29/100, Train Loss: 14927310667.8929, Val Loss: 13248671448.5171\n",
      "Epoch 30/100, Train Loss: 12156616835.4442, Val Loss: 9444359126.5183\n",
      "Epoch 31/100, Train Loss: 10667731895.9305, Val Loss: 9436097564.4697\n",
      "Epoch 32/100, Train Loss: 10190642883.4711, Val Loss: 11904080954.5071\n",
      "Epoch 33/100, Train Loss: 10072569481.1939, Val Loss: 8759104360.2452\n",
      "Epoch 34/100, Train Loss: 10113520118.6209, Val Loss: 10986503970.7327\n",
      "Epoch 35/100, Train Loss: 10109825436.9926, Val Loss: 9531880947.1918\n",
      "Epoch 36/100, Train Loss: 10039032712.5923, Val Loss: 8689675547.3801\n",
      "Epoch 37/100, Train Loss: 10061985601.5810, Val Loss: 8872448985.2774\n",
      "Epoch 38/100, Train Loss: 10060702862.1872, Val Loss: 9365093869.4853\n",
      "Epoch 39/100, Train Loss: 10037623535.5092, Val Loss: 8807147877.3449\n",
      "Epoch 40/100, Train Loss: 10052166759.8478, Val Loss: 8771992631.0268\n",
      "Epoch 41/100, Train Loss: 9971255930.9783, Val Loss: 8945744795.5722\n",
      "Epoch 42/100, Train Loss: 9937920257.2601, Val Loss: 10220719313.9797\n",
      "Epoch 43/100, Train Loss: 9982943906.8305, Val Loss: 8574378907.1019\n",
      "Epoch 44/100, Train Loss: 9931728819.3605, Val Loss: 8686352626.7763\n",
      "Epoch 45/100, Train Loss: 9979556809.9204, Val Loss: 8977163237.3488\n",
      "Epoch 46/100, Train Loss: 9952252728.2685, Val Loss: 8524581203.5357\n",
      "Epoch 47/100, Train Loss: 9956045894.8192, Val Loss: 8866069605.4625\n",
      "Epoch 48/100, Train Loss: 9894281106.7748, Val Loss: 8793449742.3054\n",
      "Epoch 49/100, Train Loss: 9945277258.6347, Val Loss: 10252973071.7399\n",
      "Epoch 50/100, Train Loss: 9930992050.9411, Val Loss: 8522816537.9927\n",
      "Epoch 51/100, Train Loss: 9931626922.6320, Val Loss: 8481765912.6288\n",
      "Epoch 52/100, Train Loss: 9929621495.6909, Val Loss: 8849881636.7629\n",
      "Epoch 53/100, Train Loss: 9860345752.0660, Val Loss: 9478300990.4029\n",
      "Epoch 54/100, Train Loss: 9905816654.1053, Val Loss: 9686108759.0239\n",
      "Epoch 55/100, Train Loss: 9911634522.4945, Val Loss: 8741674824.2638\n",
      "Epoch 56/100, Train Loss: 9902040245.4201, Val Loss: 8717718632.5979\n",
      "Epoch 57/100, Train Loss: 9882901630.1961, Val Loss: 8729950532.1721\n",
      "Epoch 58/100, Train Loss: 9869821699.0904, Val Loss: 8733057993.6944\n",
      "Epoch 59/100, Train Loss: 9902988079.8262, Val Loss: 8390233945.0697\n",
      "Epoch 60/100, Train Loss: 9922682139.0457, Val Loss: 8762366887.3614\n",
      "Epoch 61/100, Train Loss: 9812248925.8397, Val Loss: 8799622181.1235\n",
      "Epoch 62/100, Train Loss: 9917474700.8957, Val Loss: 9413854907.3890\n",
      "Epoch 63/100, Train Loss: 9823498214.6965, Val Loss: 8831441632.9514\n",
      "Epoch 64/100, Train Loss: 9860597967.0999, Val Loss: 8484754939.1558\n",
      "Epoch 65/100, Train Loss: 9820685664.6028, Val Loss: 8942570831.7732\n",
      "Epoch 66/100, Train Loss: 9854418556.4128, Val Loss: 8602969287.5074\n",
      "Epoch 67/100, Train Loss: 9779242303.2568, Val Loss: 8618928750.7904\n",
      "Epoch 68/100, Train Loss: 9823857801.3271, Val Loss: 9147221658.6551\n",
      "Epoch 69/100, Train Loss: 9842284554.6254, Val Loss: 8487598679.4942\n",
      "Epoch 70/100, Train Loss: 9843984048.5444, Val Loss: 9778511417.3940\n",
      "Epoch 71/100, Train Loss: 9829692635.7203, Val Loss: 8609821661.6357\n",
      "Epoch 72/100, Train Loss: 9755835808.2437, Val Loss: 8630007978.1911\n",
      "Epoch 73/100, Train Loss: 9818155939.3459, Val Loss: 9106661953.3894\n",
      "Epoch 74/100, Train Loss: 9802010837.1514, Val Loss: 9373686133.3043\n",
      "Early stopping after 74 epochs\n"
     ]
    }
   ],
   "source": [
    "# Create a validation set from the training data\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping after {epoch+1} epochs')\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_pred_train = model(X_train_tensor.to(device)).cpu().numpy().flatten()\n",
    "    nn_pred_test = model(X_test_tensor.to(device)).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e7ded92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set evaluation:\n",
      "\n",
      "Neural Network Evaluation Metrics:\n",
      "MSE: 8647817847.78\n",
      "RMSE: 92993.64\n",
      "MAE: 69613.55\n",
      "R²: 0.7336\n",
      "\n",
      "Test set evaluation:\n",
      "\n",
      "Neural Network Evaluation Metrics:\n",
      "MSE: 9137940629.08\n",
      "RMSE: 95592.58\n",
      "MAE: 69576.80\n",
      "R²: 0.7184\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Metrics:\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nTraining set evaluation:\")\n",
    "nn_train_metrics = evaluate_model(y_train, nn_pred_train, \"Neural Network\")\n",
    "print(\"\\nTest set evaluation:\")\n",
    "nn_test_metrics = evaluate_model(y_test, nn_pred_test, \"Neural Network\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
